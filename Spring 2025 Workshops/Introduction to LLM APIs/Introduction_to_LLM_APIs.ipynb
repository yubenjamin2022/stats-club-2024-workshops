{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "# Introduction to LLM APIs\n",
        "\n",
        "#### UCLA Statistics Club 2025\n",
        "\n",
        "APIs are ways where we can access state-of-the-art LLMs and use them for our own applications.\n"
      ],
      "metadata": {
        "id": "qBE4v92zocPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Credit to Kaggle & Google for providing the code for this notebook."
      ],
      "metadata": {
        "id": "pnc8cIsxew7P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FN7u6AriPZa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37bd1d6d-81bc-4fd3-cd3a-b273a886ef9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.7/144.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -U -q \"google-genai==1.7.0\" \"chromadb==0.6.3\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A lot of this code, you won't need to worry about. You can search up the code yourself if you want to understand what each section is responsible for."
      ],
      "metadata": {
        "id": "NUfeOOKEdAKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.api_core import retry\n",
        "from IPython.display import Markdown"
      ],
      "metadata": {
        "id": "XhfyywuHc2Uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# essentially makes it so we don't have to worry about the quotas\n",
        "\n",
        "is_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n",
        "\n",
        "genai.models.Models.generate_content = retry.Retry(\n",
        "    predicate=is_retriable)(genai.models.Models.generate_content)"
      ],
      "metadata": {
        "id": "mNUz07BVc578"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = \"API_KEY\" # adjust this to add your own API key"
      ],
      "metadata": {
        "id": "qQgJQf7NgLd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = genai.Client(api_key=API_KEY) # this connects you to the API for the LLM"
      ],
      "metadata": {
        "id": "T7lD5r4Tc86y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are some of the other models you can try loading in on your own time to try."
      ],
      "metadata": {
        "id": "b_hwfalQotIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for model in client.models.list():\n",
        "  print(model.name)"
      ],
      "metadata": {
        "id": "oy3TyZgB0lN3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e61e3d1e-5fa7-4ec1-abac-cabb03f5a079"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/chat-bison-001\n",
            "models/text-bison-001\n",
            "models/embedding-gecko-001\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro-vision\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-001\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash-001\n",
            "models/gemini-1.5-flash-001-tuning\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-1.5-flash-8b-exp-0827\n",
            "models/gemini-1.5-flash-8b-exp-0924\n",
            "models/gemini-2.5-pro-exp-03-25\n",
            "models/gemini-2.5-pro-preview-03-25\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/learnlm-1.5-pro-experimental\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/gemini-embedding-exp-03-07\n",
            "models/gemini-embedding-exp\n",
            "models/aqa\n",
            "models/imagen-3.0-generate-002\n",
            "models/gemini-2.0-flash-live-001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic prompting\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Here is some basic code to try prompting the API."
      ],
      "metadata": {
        "id": "oPxY0WOXscKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\", # you can choose from several different google Gemini models\n",
        "    contents=\"Explain what LLMs are.\") # what your input is prior to its response\n",
        "\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "id": "ZR1GwR7hsmYD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 911
        },
        "outputId": "60e3ef67-8d64-442b-ec19-b7a5c308da80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "LLMs, or **Large Language Models**, are a type of artificial intelligence model designed to understand, generate, and manipulate human language.  They are considered \"large\" because they are trained on massive datasets of text and code, often containing billions of parameters (the variables the model learns). This massive training allows them to learn complex patterns and relationships within the data, enabling them to perform a wide range of language-related tasks.\n\nHere's a breakdown of key aspects:\n\n*   **What they are:**  At their core, LLMs are advanced statistical models. They learn to predict the probability of the next word in a sequence based on the preceding words.  Think of it like autocomplete on steroids, but instead of just predicting the next word, it can generate entire paragraphs, articles, or even code.\n\n*   **How they work (in simplified terms):**\n\n    1.  **Training Data:**  LLMs are trained on massive datasets of text and code scraped from the internet, books, articles, and more.  This data is used to teach the model the relationships between words and phrases.\n    2.  **Neural Networks:** They use deep learning techniques, specifically transformer-based neural networks.  Transformers are particularly good at handling sequential data like language because they can consider the context of words in a sentence, not just the immediately preceding words.\n    3.  **Parameters:** The \"large\" in LLM refers to the number of parameters in the neural network.  More parameters generally allow the model to learn more complex patterns.\n    4.  **Prediction:** When you give an LLM a prompt (e.g., a question, a starting sentence, or a request), it uses its learned knowledge to predict the most likely sequence of words to follow. It iteratively generates words, feeding each new word back into the model to predict the next.\n    5.  **Fine-tuning (Optional):** After the initial training, LLMs can be fine-tuned on specific datasets or tasks to improve their performance in a particular domain (e.g., medical text, legal documents, coding).  This allows them to become more specialized.\n\n*   **What they can do:**  LLMs can perform a wide variety of natural language processing (NLP) tasks, including:\n\n    *   **Text Generation:**  Writing articles, poems, scripts, emails, and other types of creative content.\n    *   **Translation:**  Translating text between different languages.\n    *   **Question Answering:**  Answering questions based on provided context or general knowledge.\n    *   **Summarization:**  Creating concise summaries of long documents or articles.\n    *   **Code Generation:**  Writing code in various programming languages.\n    *   **Text Completion:**  Completing partially written text.\n    *   **Sentiment Analysis:**  Determining the emotional tone of a piece of text.\n    *   **Chatbots:**  Powering conversational AI applications.\n    *   **Content Moderation:**  Identifying and flagging inappropriate content.\n\n*   **Examples of LLMs:** Some popular examples of LLMs include:\n\n    *   **GPT (Generative Pre-trained Transformer) series:**  Developed by OpenAI (e.g., GPT-3, GPT-4) - known for their impressive text generation abilities.\n    *   **BERT (Bidirectional Encoder Representations from Transformers):**  Developed by Google - excels at understanding the context of words in a sentence.\n    *   **LaMDA (Language Model for Dialogue Applications):**  Developed by Google - designed for conversational AI.\n    *   **Llama (Large Language Model Meta AI):**  Developed by Meta AI (Facebook).\n    *   **Bard (Google's conversational AI service):** Powered by Google's LaMDA and more recent models.\n\n*   **Limitations:**\n\n    *   **Lack of True Understanding:**  LLMs don't truly \"understand\" the meaning of words in the same way humans do. They are essentially sophisticated pattern-matching machines.\n    *   **Bias:**  LLMs can inherit biases from their training data, leading to biased or unfair outputs.\n    *   **Hallucinations:**  LLMs can sometimes generate inaccurate or nonsensical information, often presented as factual (this is called \"hallucination\").\n    *   **Computational Cost:** Training and running LLMs require significant computational resources and energy.\n    *   **Ethical Concerns:**  There are ethical concerns related to the misuse of LLMs, such as spreading misinformation, creating deepfakes, and automating jobs.\n    *   **Context Window Limitations:**  Most LLMs have a limit on the amount of text they can consider at one time (the \"context window\"). This can affect their ability to handle very long conversations or complex tasks that require remembering a lot of information.\n\nIn summary, LLMs are powerful tools with the ability to process and generate human language. They are rapidly evolving and have the potential to transform many industries, but it's important to be aware of their limitations and potential ethical implications.\n"
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if you want to include history of the conversation, you can also add this in so the LLM can reference chat history\n",
        "\n",
        "chat = client.chats.create(model='gemini-2.0-flash', history=[])\n",
        "response = chat.send_message('Hello! My name is Ben.')\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "id": "Yjp5gTEnh36U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "635448bc-9ac9-4c4f-fff6-5b4e48f48464"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Hello Ben! It's nice to meet you. How can I help you today?\n"
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message('Tell me what APIs are.')\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "id": "nkLZZquB0a7_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 924
        },
        "outputId": "bff3ac08-01f8-45b4-d281-ef6c6715de14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, Ben, let's break down what APIs are.\n\n**API stands for Application Programming Interface.** Think of it as a set of rules and specifications that allow different software applications to communicate and exchange data with each other.\n\nHere's a more detailed explanation using analogies:\n\n**Analogy 1: The Restaurant Menu**\n\n*   Imagine you're at a restaurant. You don't go into the kitchen and start cooking yourself. Instead, you use the **menu (the API)** to tell the kitchen what you want.\n*   The **menu lists the dishes (the functions/data available)**.\n*   You **order a specific dish (make an API request)**.\n*   The **kitchen prepares the dish (performs the requested function)**.\n*   The **waiter brings you the finished dish (the API returns data)**.\n\n**Analogy 2: The Electrical Outlet**\n\n*   You have various electrical devices (lamps, toasters, computers).\n*   They all use a standard **electrical outlet (the API)**.\n*   You **plug your device into the outlet (make an API call)**.\n*   The **power company provides electricity (the underlying service)**.\n*   Your **device receives power and works (the data/function is provided)**.\n\n**In Technical Terms:**\n\n*   **Application:**  A piece of software (e.g., a mobile app, a website, a desktop program).\n*   **Programming Interface:**  A set of protocols, routines, and tools for building software applications.  It specifies how software components should interact.\n\n**Key Concepts of APIs:**\n\n*   **Requests:**  An application sends a request to the API to ask for specific data or to perform a specific action.\n*   **Responses:** The API processes the request and sends back a response, which typically includes the requested data or confirmation that the action was performed.\n*   **Endpoints:**  These are specific URLs (web addresses) that represent particular resources or functions offered by the API.  Think of them as different pages in a website, but for machines.\n*   **Data Formats:** APIs often use standard data formats like JSON (JavaScript Object Notation) or XML (Extensible Markup Language) to transmit data. These formats are human-readable and easily parsed by computers.\n*   **Authentication:**  Many APIs require authentication to ensure that only authorized applications can access them.  This often involves using API keys or OAuth tokens.\n\n**Why are APIs important?**\n\n*   **Interoperability:** APIs allow different systems and applications to work together seamlessly, even if they are built using different technologies.\n*   **Modularity:** APIs allow developers to build applications from reusable components, making development faster and more efficient.\n*   **Innovation:** APIs enable developers to create new and innovative applications by combining the functionality of different services.\n*   **Data Access:** APIs provide a controlled and secure way to access data from various sources.\n*   **Microservices Architecture:** APIs are fundamental to microservices, where an application is built as a collection of small, independent services communicating via APIs.\n\n**Examples of APIs:**\n\n*   **Google Maps API:**  Allows developers to embed Google Maps into their websites or applications.\n*   **Twitter API:**  Allows developers to access and interact with Twitter data (e.g., retrieve tweets, post tweets, follow users).\n*   **Payment APIs (e.g., Stripe, PayPal):**  Allow developers to integrate payment processing into their applications.\n*   **Weather APIs:**  Provide weather data (e.g., temperature, humidity, forecast) to applications.\n*   **Database APIs:** Allow applications to interact with databases to retrieve, store, and update data.\n\n**In summary, an API is a middleman that allows different software systems to talk to each other, enabling them to share data and functionality in a structured and controlled way.**\n\nI hope this explanation is helpful, Ben!  Let me know if you have any more questions.  We can dive deeper into specific types of APIs, how to use them, or anything else related to the topic.\n"
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat.send_message('What is my name?')\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "id": "NnS0Ub280e1W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "c21d0204-839b-4c68-ce8e-82043b528781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Your name is Ben. You told me at the beginning of our conversation.\n"
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adjusting configurations\n",
        "\n",
        "We can adjust some of our configurations to change some parameters of the model."
      ],
      "metadata": {
        "id": "zEJOtQwK7SAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = types.GenerateContentConfig(max_output_tokens=200,\n",
        "                                           temperature = 0.02, # mess around with it\n",
        "                                            top_p=0.95) # this too\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=model_config,\n",
        "\n",
        "    contents='List your top 100 favorite colors.')\n",
        "\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "id": "POTEx7Cx7Ra9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "outputId": "101329cd-96c6-40b1-d76a-ba220b8de928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, this is a fun challenge! Listing 100 *favorite* colors is subjective and might be hard to do in a way that's truly distinct for each one. I'll aim for a diverse range, using descriptive names and trying to avoid too much repetition within color families. I'll also try to include some less common or more nuanced shades.\n\nHere's my attempt at a list of 100 favorite colors, in no particular order:\n\n1.  **Cerulean Blue:** A bright, sky-like blue.\n2.  **Forest Green:** Deep, rich green of a dense forest.\n3.  **Crimson Red:** A strong, slightly bluish-red.\n4.  **Golden Yellow:** Warm and radiant, like sunlight.\n5.  **Lavender Purple:** Soft, calming, and floral.\n6.  **Teal:** A mix of blue and green, often with a hint of gray"
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zero-shot and few-shot prompting\n",
        "\n",
        "We can prompt our LLM with some instructions and adjust some configurations to get a desired result"
      ],
      "metadata": {
        "id": "HD1QCb-l7m-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Zero shot\n",
        "\n",
        "model_config = types.GenerateContentConfig(\n",
        "    temperature=0.1,\n",
        "    top_p=1,\n",
        "    max_output_tokens=5,\n",
        ")\n",
        "\n",
        "zero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
        "Review: \"Her\" is a disturbing study revealing the direction\n",
        "humanity is headed if AI is allowed to keep evolving,\n",
        "unchecked. I wish there were more movies like this masterpiece.\n",
        "Sentiment: \"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=model_config,\n",
        "    contents=zero_shot_prompt)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "K5nXK-FG7mdK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e2c8e9a-d2b0-4e91-a760-868961ffb44c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POSITIVE\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the enum package, you can restrict the output of the text to a select few values."
      ],
      "metadata": {
        "id": "Y9Jds3PITB_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enum\n",
        "\n",
        "class Sentiment(enum.Enum):\n",
        "    POSITIVE = \"positive\"\n",
        "    NEUTRAL = \"neutral\"\n",
        "    NEGATIVE = \"negative\"\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=types.GenerateContentConfig(\n",
        "        response_mime_type=\"text/x.enum\",\n",
        "        response_schema=Sentiment\n",
        "    ),\n",
        "    contents=zero_shot_prompt)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "IDEitDCoG1Qk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c79cdec7-454b-4699-a528-97e450b5e77c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Few shot offers a couple of examples, giving it better performance over the proposed task."
      ],
      "metadata": {
        "id": "VFQ24aikTeFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
        "\n",
        "EXAMPLE:\n",
        "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"small\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"cheese\", \"tomato sauce\", \"pepperoni\"]\n",
        "}\n",
        "```\n",
        "\n",
        "EXAMPLE:\n",
        "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"large\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
        "}\n",
        "```\n",
        "\n",
        "ORDER:\n",
        "\"\"\"\n",
        "\n",
        "customer_order = \"Give me a large with cheese & pineapple\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=types.GenerateContentConfig(\n",
        "        temperature=0.1,\n",
        "        top_p=1,\n",
        "        max_output_tokens=250,\n",
        "    ),\n",
        "    contents=[few_shot_prompt, customer_order])\n",
        "\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "id": "C2b-qJRgTdnP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "a977ca68-051c-40a0-84e9-088c8a2a7c94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```json\n{\n\"size\": \"large\",\n\"type\": \"normal\",\n\"ingredients\": [\"cheese\", \"pineapple\"]\n}\n```\n"
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can likewise restrict the output to a certain type, in this case a json file."
      ],
      "metadata": {
        "id": "V3OMFVThTx7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import typing_extensions as typing\n",
        "\n",
        "class PizzaOrder(typing.TypedDict):\n",
        "    size: str\n",
        "    ingredients: list[str]\n",
        "    type: str\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=types.GenerateContentConfig(\n",
        "        temperature=0.1,\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=PizzaOrder,\n",
        "    ),\n",
        "    contents=\"Can I have a large dessert pizza with apple and chocolate\")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "h2pVJfAnTxcG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72ced3ac-2d58-476b-9698-08f6ef1dcffa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"size\": \"large\",\n",
            "  \"ingredients\": [\"apple\", \"chocolate\"],\n",
            "  \"type\": \"dessert\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chain-of-thought prompting\n",
        "\n",
        "A main issue with LLMs is that they output the next most probable token autoregressively, and if the data they are trained on is faulty, they are prone to hallucinate (they output false information). One solution to this is chain-of-thought prompting, where we ask the LLM to write out their steps to a their solution, and it improves the accuracy of their generation."
      ],
      "metadata": {
        "id": "d8CwK6Bi8ZjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
        "am 20 years old. How old is my partner? Return the answer directly.\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    contents=prompt)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "98cZ1gYq8ZHa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bbdd218-c121-4363-c052-fb7a711fa105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we now tell the model to think about it step by step\n",
        "\n",
        "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\n",
        "I am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    contents=prompt)\n",
        "\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "id": "MiJfgwHxG6oF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "4d9b1118-9fd8-414d-bb6f-e32d9063e633"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here's how to solve the problem:\n\n1. **Find the age difference:** When you were 4, your partner was 3 times your age, meaning they were 4 * 3 = 12 years old.\n2. **Calculate the age difference:** The age difference between you and your partner is 12 - 4 = 8 years.\n3. **Determine the partner's current age:** Since the age difference remains constant, your partner is currently 20 + 8 = 28 years old.\n\n**Answer:** Your partner is 28 years old.\n"
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval Augmented Generation\n",
        "\n",
        "Retrieval Augmented Generation (RAG) is a method to incorporate various data (in this case text, but could be other mediums) into our LLMs as a reference, allowing us to input the most modern data\n"
      ],
      "metadata": {
        "id": "KUsbjx4JUENj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here are some documents to get us started (provided by Google)\n",
        "\n",
        "DOCUMENT1 = \"Operating the Climate Control System  Your Googlecar has a climate control system that allows you to adjust the temperature and airflow in the car. To operate the climate control system, use the buttons and knobs located on the center console.  Temperature: The temperature knob controls the temperature inside the car. Turn the knob clockwise to increase the temperature or counterclockwise to decrease the temperature. Airflow: The airflow knob controls the amount of airflow inside the car. Turn the knob clockwise to increase the airflow or counterclockwise to decrease the airflow. Fan speed: The fan speed knob controls the speed of the fan. Turn the knob clockwise to increase the fan speed or counterclockwise to decrease the fan speed. Mode: The mode button allows you to select the desired mode. The available modes are: Auto: The car will automatically adjust the temperature and airflow to maintain a comfortable level. Cool: The car will blow cool air into the car. Heat: The car will blow warm air into the car. Defrost: The car will blow warm air onto the windshield to defrost it.\"\n",
        "DOCUMENT2 = 'Your Googlecar has a large touchscreen display that provides access to a variety of features, including navigation, entertainment, and climate control. To use the touchscreen display, simply touch the desired icon.  For example, you can touch the \"Navigation\" icon to get directions to your destination or touch the \"Music\" icon to play your favorite songs.'\n",
        "DOCUMENT3 = \"Shifting Gears Your Googlecar has an automatic transmission. To shift gears, simply move the shift lever to the desired position.  Park: This position is used when you are parked. The wheels are locked and the car cannot move. Reverse: This position is used to back up. Neutral: This position is used when you are stopped at a light or in traffic. The car is not in gear and will not move unless you press the gas pedal. Drive: This position is used to drive forward. Low: This position is used for driving in snow or other slippery conditions.\"\n",
        "\n",
        "documents = [DOCUMENT1, DOCUMENT2, DOCUMENT3]"
      ],
      "metadata": {
        "id": "U1OAyrHnT7bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
        "\n",
        "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
        "    # Specify whether to generate embeddings for documents, or queries\n",
        "    document_mode = True\n",
        "\n",
        "    @retry.Retry(predicate=is_retriable)\n",
        "    def __call__(self, input: Documents) -> Embeddings:\n",
        "        if self.document_mode:\n",
        "            embedding_task = \"retrieval_document\"\n",
        "        else:\n",
        "            embedding_task = \"retrieval_query\"\n",
        "\n",
        "        response = client.models.embed_content(\n",
        "            model=\"models/text-embedding-004\", # can change\n",
        "            contents=input,\n",
        "            config=types.EmbedContentConfig(\n",
        "                task_type=embedding_task,\n",
        "            ),\n",
        "        )\n",
        "        return [e.values for e in response.embeddings]"
      ],
      "metadata": {
        "id": "CSq_Itz8WIFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the embedding models you can use to embed your data."
      ],
      "metadata": {
        "id": "1clF_nbEWd6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for m in client.models.list():\n",
        "    if \"embedContent\" in m.supported_actions:\n",
        "        print(m.name)"
      ],
      "metadata": {
        "id": "ZT8X_gu1WbCk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff896662-3165-4a34-bf70-16d42c06e6e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/gemini-embedding-exp-03-07\n",
            "models/gemini-embedding-exp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "\n",
        "DB_NAME = \"googlecardb\"\n",
        "\n",
        "embed_fn = GeminiEmbeddingFunction()\n",
        "embed_fn.document_mode = True\n",
        "\n",
        "chroma_client = chromadb.Client()\n",
        "db = chroma_client.get_or_create_collection(name=DB_NAME, embedding_function=embed_fn)\n",
        "\n",
        "db.add(documents=documents, ids=[str(i) for i in range(len(documents))])"
      ],
      "metadata": {
        "id": "3Ocd0Pvdfiyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db.count()"
      ],
      "metadata": {
        "id": "C61je6jbfl8U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3ce902d-f48e-4c82-c604-a8106fef21a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "db.peek(1)"
      ],
      "metadata": {
        "id": "rzMjk8rxfnRQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d88aef05-1e52-4f34-8ea6-a7b0a26b745c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ids': ['0'],\n",
              " 'embeddings': array([[ 1.89996641e-02,  7.50530604e-03, -2.69203484e-02,\n",
              "         -9.78849642e-03, -8.69853329e-03, -1.27060898e-02,\n",
              "          2.98559777e-02,  6.81265350e-03, -5.14574721e-03,\n",
              "          3.52885611e-02, -8.06997046e-02,  7.53531009e-02,\n",
              "          8.58849138e-02,  1.30070690e-02, -2.77891336e-03,\n",
              "         -1.05936602e-01, -3.98958661e-03, -7.90926255e-03,\n",
              "         -6.78334385e-02,  9.37942939e-04, -3.31661627e-02,\n",
              "          2.91897804e-02, -4.37331311e-02, -2.03247666e-02,\n",
              "         -4.03934792e-02, -4.03673872e-02,  4.37083244e-02,\n",
              "          4.35296260e-02, -5.30568957e-02, -7.60380086e-03,\n",
              "          1.10596254e-01,  2.93870177e-02, -2.69055367e-03,\n",
              "         -2.77709514e-02,  3.56521569e-02,  8.06248095e-03,\n",
              "         -6.98892726e-03, -4.19588238e-02, -1.22357225e-02,\n",
              "         -7.43979216e-02, -8.68393779e-02,  1.45487059e-02,\n",
              "          1.64316818e-02,  4.95274737e-02,  5.96513413e-03,\n",
              "         -3.29070538e-02, -4.59746048e-02,  6.05700687e-02,\n",
              "          3.69714126e-02, -3.67843956e-02,  3.23379971e-02,\n",
              "          5.57416072e-03, -2.60148998e-02,  3.72896157e-02,\n",
              "          1.09929522e-03,  2.22460013e-02, -3.41857560e-02,\n",
              "         -1.33831445e-02,  3.84481438e-02,  1.01510370e-02,\n",
              "         -4.13358212e-02,  1.50308516e-02, -4.16338220e-02,\n",
              "          2.77164355e-02,  1.77828148e-02, -5.60168698e-02,\n",
              "         -1.28984069e-02, -4.63326536e-02, -4.68938565e-03,\n",
              "         -1.61614176e-02,  4.64589000e-02,  4.97067757e-02,\n",
              "         -1.76815037e-02,  2.75055915e-02, -4.77083847e-02,\n",
              "         -3.02048232e-02, -3.58368531e-02, -1.49208391e-02,\n",
              "          1.64871980e-02,  5.08249328e-02, -2.80048344e-02,\n",
              "          6.99736923e-02,  5.95696792e-02,  3.21656168e-02,\n",
              "         -9.67071112e-03, -2.01593228e-02,  6.69856966e-02,\n",
              "          1.51496977e-02, -7.96246305e-02, -4.13303310e-03,\n",
              "          7.46189281e-02,  6.72254059e-03, -1.84513796e-02,\n",
              "          2.87920292e-02,  7.01305121e-02,  1.55893695e-02,\n",
              "         -1.16371922e-01, -7.99548775e-02,  5.50818779e-02,\n",
              "          7.43377358e-02,  2.68951990e-02,  5.65322861e-02,\n",
              "         -1.09314676e-02, -4.97194529e-02,  2.15398204e-02,\n",
              "          2.49629170e-02,  9.70174186e-03, -2.00729957e-03,\n",
              "         -5.23507595e-02,  5.45086190e-02,  2.20851172e-02,\n",
              "         -5.07799238e-02,  1.51937539e-02,  3.40860412e-02,\n",
              "          2.94148624e-02, -2.27203965e-02, -4.44913059e-02,\n",
              "          4.19270061e-02,  1.33883394e-02, -8.03010166e-03,\n",
              "         -3.02138049e-02, -1.52768716e-02, -1.79337766e-02,\n",
              "          4.12679724e-02,  4.28794511e-02,  1.86120179e-02,\n",
              "          6.62900433e-02,  1.68055836e-02, -7.44678685e-03,\n",
              "         -7.88552612e-02, -1.01826247e-03, -5.30414023e-02,\n",
              "         -1.77297127e-02,  4.33610976e-02, -2.87686586e-02,\n",
              "          4.37279977e-02,  6.51901439e-02,  1.42448116e-02,\n",
              "          3.04472726e-02, -1.51222004e-02,  9.45043378e-03,\n",
              "         -1.39130075e-02, -7.50348866e-02,  7.15902681e-03,\n",
              "         -2.02401206e-02,  1.58404314e-03,  2.50295680e-02,\n",
              "         -2.19853669e-02, -5.25463633e-02,  2.95491908e-02,\n",
              "          2.80410163e-02,  1.59156360e-02,  4.83962148e-03,\n",
              "          3.42256613e-02,  2.61947587e-02, -1.79085378e-02,\n",
              "         -9.59200598e-03, -1.30515965e-02,  5.45655526e-02,\n",
              "          4.18885499e-02,  1.15598358e-01, -2.15165876e-02,\n",
              "          1.55694981e-03,  4.04374786e-02, -4.02314290e-02,\n",
              "          7.93419555e-02, -6.48735513e-05, -4.36600596e-02,\n",
              "         -1.67760327e-02, -5.54723442e-02, -1.18653001e-02,\n",
              "         -5.03605045e-02,  2.53493730e-02, -7.12577999e-02,\n",
              "          9.61738545e-03, -8.39261059e-03, -2.53669359e-02,\n",
              "         -3.75324562e-02, -2.25138683e-02,  2.07773894e-02,\n",
              "          9.74554345e-02, -3.56078707e-02, -9.07747075e-03,\n",
              "         -3.51976268e-02,  5.00649167e-03,  8.92841443e-03,\n",
              "         -2.23056767e-02,  1.60871912e-02,  4.47834916e-02,\n",
              "          1.94993690e-02,  4.22408767e-02,  4.36633378e-02,\n",
              "          4.17801514e-02,  6.61993539e-03,  3.28581221e-03,\n",
              "          1.34587772e-02,  4.60674651e-02, -5.68813039e-03,\n",
              "          2.91601587e-02, -3.89970653e-02,  2.51733679e-02,\n",
              "          2.41864398e-02, -2.06118114e-02, -8.49481300e-03,\n",
              "         -7.96157196e-02,  3.14108934e-03, -3.03156907e-04,\n",
              "         -7.82509372e-02, -5.01308031e-03,  5.65630989e-03,\n",
              "          1.24495151e-02, -1.11163808e-02, -2.71381363e-02,\n",
              "         -2.70434972e-02, -6.62122518e-02,  1.71623491e-02,\n",
              "          1.83970556e-02, -3.10235117e-02,  2.75832936e-02,\n",
              "         -3.41743082e-02,  7.28364475e-03, -5.92893362e-02,\n",
              "          1.31034762e-01,  2.93862680e-03,  3.27316076e-02,\n",
              "          2.23102737e-02, -4.07483578e-02,  1.35542676e-02,\n",
              "         -4.49327976e-02, -6.06717588e-03, -3.24808666e-03,\n",
              "          2.87276991e-02, -2.08841208e-02,  3.97731829e-03,\n",
              "         -7.79294968e-03,  4.93008830e-02,  4.95140441e-02,\n",
              "         -3.70474011e-02,  3.40877101e-02,  7.44339498e-03,\n",
              "          2.74706036e-02,  4.58842888e-02,  4.23083864e-02,\n",
              "          2.34148689e-02,  1.48451021e-02, -2.37723533e-02,\n",
              "          6.47631735e-02,  1.57277603e-02, -4.26457748e-02,\n",
              "         -8.36548060e-02, -3.28711458e-02, -3.68208289e-02,\n",
              "         -2.07035076e-02,  4.69575636e-03, -3.04315165e-02,\n",
              "         -2.19081286e-02, -3.13257449e-03,  3.33950296e-02,\n",
              "          3.21173221e-02, -1.72844790e-02,  3.07742245e-02,\n",
              "          3.91557366e-02, -3.57035585e-02,  5.66983642e-03,\n",
              "          1.98001713e-02, -1.09939367e-01, -3.80607173e-02,\n",
              "          9.85682476e-03, -5.44885881e-02, -2.41438188e-02,\n",
              "          2.44294759e-02,  1.77268439e-03,  2.96395691e-03,\n",
              "         -2.12335661e-02, -1.25241298e-02,  1.70172658e-02,\n",
              "         -6.21883534e-02, -1.07621728e-02, -5.09412214e-03,\n",
              "         -1.35658048e-02,  3.85766551e-02,  5.11570498e-02,\n",
              "          2.18521468e-02, -4.14995439e-02,  3.34418379e-02,\n",
              "         -3.93004157e-02,  6.15507318e-03,  3.12885921e-03,\n",
              "         -6.00040518e-02,  1.04700611e-03,  3.84333506e-02,\n",
              "         -1.19297365e-02, -3.65912318e-02, -4.38246727e-02,\n",
              "          5.30170575e-02, -9.58478730e-03,  4.97351848e-02,\n",
              "          1.51532618e-02,  1.89138837e-02, -4.18482982e-02,\n",
              "          1.52532328e-02,  4.77849133e-02,  6.88652974e-03,\n",
              "          2.96370517e-02,  4.28295061e-02, -4.20004539e-02,\n",
              "          5.03814965e-03, -2.89589725e-02, -9.47450101e-03,\n",
              "         -7.19804608e-04, -2.04819646e-02,  1.71097741e-02,\n",
              "         -1.69946812e-02, -1.51602775e-02,  1.40648400e-02,\n",
              "          4.33577523e-02, -1.28116727e-01,  1.96245406e-02,\n",
              "         -9.79382778e-04, -1.49972932e-02,  3.11941318e-02,\n",
              "         -3.98269072e-02, -3.17095779e-02, -1.03539908e-02,\n",
              "          2.90332790e-02,  1.71464738e-02, -2.18943600e-02,\n",
              "         -1.20792107e-03,  2.02216711e-02, -5.82920201e-02,\n",
              "          8.70636450e-06, -2.70486567e-02, -8.37526023e-02,\n",
              "         -3.73691204e-03, -7.00314045e-02,  3.47714312e-02,\n",
              "         -1.65883601e-02,  3.71062458e-02,  8.21944419e-03,\n",
              "          3.54773812e-02,  1.61994863e-02,  7.05077574e-02,\n",
              "          8.83539952e-03,  2.18494236e-02, -5.50299250e-02,\n",
              "          1.19913522e-04,  3.50320153e-02,  4.99191508e-02,\n",
              "          2.08908767e-02, -1.21140964e-02,  3.34130600e-02,\n",
              "          3.32010910e-02,  4.11024615e-02,  1.52024766e-02,\n",
              "         -2.07796833e-03, -4.94461246e-02,  5.46362549e-02,\n",
              "         -1.91538725e-02,  4.89460416e-02, -3.06148790e-02,\n",
              "         -1.46947559e-02,  2.59972401e-02,  1.89346366e-03,\n",
              "         -8.85396544e-03,  1.43837566e-02, -2.83453707e-02,\n",
              "          2.60757376e-02, -7.98581075e-03,  1.98935997e-02,\n",
              "          1.55275883e-02, -8.87474231e-03,  2.31279954e-02,\n",
              "          3.03729586e-02,  2.14788988e-02,  3.26980092e-03,\n",
              "          3.87281664e-02, -1.32220760e-02,  1.12496624e-02,\n",
              "         -5.03361458e-03, -4.75973040e-02, -1.78676229e-02,\n",
              "         -5.71023077e-02,  3.11246980e-02, -5.31051680e-03,\n",
              "          1.57550387e-02,  5.20151109e-02, -5.72069474e-02,\n",
              "          5.15301060e-03,  3.29970457e-02,  2.26406157e-02,\n",
              "         -3.07078045e-02,  1.87925640e-02, -1.86993480e-02,\n",
              "          2.33180430e-02, -1.82738602e-02, -2.46863026e-04,\n",
              "         -8.52621868e-02,  2.53148209e-02,  2.06176797e-03,\n",
              "         -3.89578417e-02, -2.23115413e-03, -4.10256907e-02,\n",
              "          4.55508120e-02, -7.24424720e-02,  5.08268876e-03,\n",
              "          4.42190096e-02,  1.73661846e-03,  1.63513031e-02,\n",
              "          9.09441058e-03,  1.04114031e-02, -4.86889202e-03,\n",
              "         -2.63646524e-02,  7.87991844e-03,  8.30337685e-03,\n",
              "         -1.01753809e-02, -2.47611739e-02,  6.67762756e-02,\n",
              "          4.89272997e-02,  2.38461569e-02, -6.42249808e-02,\n",
              "          8.06081109e-03,  4.79382835e-02,  6.00473769e-02,\n",
              "          2.56416928e-02, -1.06949676e-02, -3.17716524e-02,\n",
              "         -3.00298259e-02,  3.23185213e-02, -1.56965274e-02,\n",
              "          3.98179255e-02,  1.11237010e-02,  3.77057530e-02,\n",
              "         -5.90020716e-02,  6.65883161e-03,  1.55508593e-02,\n",
              "         -3.98168378e-02, -2.19614618e-03, -3.17364447e-02,\n",
              "          9.28087812e-03, -1.57921314e-02, -3.60821970e-02,\n",
              "          2.23480631e-02,  7.26326481e-02,  8.54933541e-03,\n",
              "         -1.96508467e-02,  4.01913226e-02, -3.06365900e-02,\n",
              "         -1.96762756e-02, -3.83199602e-02,  1.48206614e-02,\n",
              "         -2.02410016e-02, -1.89087112e-02,  3.50414440e-02,\n",
              "          3.49851511e-02, -1.53734237e-02, -8.05087294e-03,\n",
              "         -7.98325636e-04,  5.38129210e-02,  4.28347513e-02,\n",
              "         -2.33207271e-02,  1.76745448e-02, -3.91262732e-02,\n",
              "          2.53158361e-02, -5.43349935e-03,  3.27506177e-02,\n",
              "          1.16547355e-02,  2.72344295e-02, -4.21163514e-02,\n",
              "          1.98197179e-02, -3.02518159e-02,  6.00851811e-02,\n",
              "         -3.92581001e-02,  5.69727384e-02,  4.16435599e-02,\n",
              "         -5.45447841e-02, -8.62797443e-03,  5.73354736e-02,\n",
              "         -8.94314330e-03, -3.72394882e-02,  4.12784889e-03,\n",
              "         -1.29805785e-02,  5.85994422e-02,  4.10515368e-02,\n",
              "         -7.88140856e-03,  6.91415817e-02,  1.82892084e-02,\n",
              "         -7.59132132e-02,  3.91482785e-02,  1.00310231e-02,\n",
              "          2.27377769e-02,  3.80710373e-03,  2.31498890e-02,\n",
              "          2.42068712e-02, -1.37068238e-02, -5.82525041e-03,\n",
              "          1.76540092e-02,  5.13952859e-02, -4.77824695e-02,\n",
              "         -5.54873943e-02,  1.18202306e-02,  6.27043992e-02,\n",
              "          1.87041499e-02, -6.80633560e-02, -4.02098186e-02,\n",
              "         -1.18157398e-02,  3.17377560e-02, -4.04558517e-02,\n",
              "         -2.29886267e-02,  1.09081238e-05,  7.27420002e-02,\n",
              "         -1.16472011e-02, -2.37533189e-02, -3.38588320e-02,\n",
              "         -2.19844095e-02, -6.56050304e-03, -1.48762893e-02,\n",
              "         -4.41998839e-02,  5.02602272e-02,  3.84675451e-02,\n",
              "          1.98185090e-02, -6.06090948e-02,  2.10781377e-02,\n",
              "          4.61731991e-03,  4.19402868e-02, -5.42518981e-02,\n",
              "          2.91183051e-02,  4.73365746e-02,  1.81997810e-02,\n",
              "          1.52857509e-02, -2.08034776e-02, -5.36849052e-02,\n",
              "          6.35842606e-02,  2.73762550e-02,  4.96339686e-02,\n",
              "          3.09747737e-02,  1.01277744e-02, -3.44962217e-02,\n",
              "          4.85864840e-02,  1.51261436e-02,  1.49459867e-02,\n",
              "          4.13929597e-02, -3.76431942e-02,  1.56074986e-02,\n",
              "          1.15082236e-02,  2.90358942e-02,  2.45141797e-02,\n",
              "          2.80922279e-02, -1.28367553e-02,  3.14410753e-03,\n",
              "          6.41218573e-02, -2.22793669e-02, -5.03757261e-02,\n",
              "          4.78862412e-02,  3.36291753e-02, -5.38712293e-02,\n",
              "         -8.94516893e-03,  2.78146621e-02, -1.16451690e-02,\n",
              "         -5.78200072e-03,  1.86787676e-02,  9.93121322e-03,\n",
              "         -2.65693059e-03, -1.70143209e-02, -1.55063514e-02,\n",
              "          2.41588745e-02, -1.78989361e-03,  6.64451048e-02,\n",
              "         -1.14727626e-03, -3.28906067e-02, -1.49553595e-03,\n",
              "         -2.82162777e-03, -7.60558620e-02, -1.55637471e-03,\n",
              "         -4.91528492e-03, -2.19545607e-02,  2.75564697e-02,\n",
              "         -1.25497794e-02, -1.68638136e-02, -2.07204279e-02,\n",
              "         -2.36812923e-02, -3.07043232e-02,  6.31876988e-03,\n",
              "          3.43244486e-02,  1.70262139e-02,  4.92697842e-02,\n",
              "          2.93082576e-02,  4.10187989e-03,  2.09337920e-02,\n",
              "          7.19857663e-02,  1.05966022e-03, -1.47689087e-02,\n",
              "         -3.47245783e-02, -2.16487832e-02, -9.95141827e-03,\n",
              "          1.29853487e-02, -5.48050972e-03, -7.70190209e-02,\n",
              "          1.12167513e-02, -1.51839443e-02, -1.83910392e-02,\n",
              "         -1.97323561e-02, -1.48972301e-02,  8.46249908e-02,\n",
              "          6.35140855e-03,  2.03219000e-02, -1.94084048e-02,\n",
              "         -1.08773485e-02, -4.57313657e-02, -4.48526070e-02,\n",
              "          2.56617926e-02,  1.93536896e-02,  1.00775696e-02,\n",
              "          7.18023628e-03, -3.55099514e-02, -4.55002636e-02,\n",
              "         -7.88334943e-03, -4.93443757e-02,  2.36826204e-02,\n",
              "          5.81833301e-03, -7.79057108e-03, -1.31063825e-02,\n",
              "          2.89645623e-02,  9.53624845e-02,  4.62271878e-03,\n",
              "         -2.28673667e-02, -3.00150272e-02,  8.50995071e-03,\n",
              "          2.95229964e-02, -2.00281832e-02, -3.06392740e-02,\n",
              "          3.70190083e-03,  5.17040044e-02,  3.78289409e-02,\n",
              "         -4.52865958e-02, -3.04144323e-02, -6.33255765e-02,\n",
              "         -1.21121779e-02, -2.26585940e-03,  4.00145315e-02,\n",
              "         -7.68749863e-02, -3.96612063e-02, -2.40458176e-02,\n",
              "         -1.22921094e-02, -3.65987513e-03, -5.63489683e-02,\n",
              "         -2.04586294e-02,  2.61900318e-03, -4.19872394e-03,\n",
              "          2.29546800e-02, -3.81762721e-02, -4.06912453e-02,\n",
              "          5.58966659e-02, -2.28307825e-02, -7.99906347e-03,\n",
              "         -6.71147089e-03,  2.39457283e-02, -2.71236263e-02,\n",
              "          3.32449190e-02, -3.31203314e-03, -1.45904701e-02,\n",
              "         -1.48993256e-02, -5.38930371e-02,  8.52978975e-03,\n",
              "          2.41371319e-02,  2.83823945e-02,  2.75515486e-02,\n",
              "         -9.44455899e-03,  2.91343611e-02,  7.87780806e-03,\n",
              "         -3.50229479e-02, -1.73738748e-02,  6.11178856e-03,\n",
              "         -5.17762974e-02,  4.22114553e-03, -7.85780549e-02,\n",
              "         -1.67391486e-02,  2.43430994e-02, -1.91075541e-02,\n",
              "          6.80573098e-03, -8.85819551e-03,  1.04752835e-02,\n",
              "          3.03049795e-02,  2.26538684e-02, -8.99459701e-03,\n",
              "          3.97454314e-02, -1.19835865e-02,  5.36519326e-02,\n",
              "         -1.73529275e-02,  1.60130672e-02,  5.74031807e-02,\n",
              "          1.70067605e-02, -4.71418053e-02, -1.67144754e-03,\n",
              "          5.65098748e-02,  2.57398672e-02,  4.74409908e-02,\n",
              "         -6.82199048e-03,  2.76359431e-02, -3.98197398e-02,\n",
              "         -7.07716541e-03,  5.33328727e-02, -2.15630159e-02,\n",
              "         -6.25305101e-02, -6.47531971e-02,  9.41584911e-03,\n",
              "          3.55411954e-02,  5.78559935e-02,  1.71727333e-02,\n",
              "         -1.82200577e-02,  9.01891442e-04, -3.21503207e-02,\n",
              "          3.91423702e-03,  3.67967710e-02,  2.71635167e-02,\n",
              "          1.97215788e-02,  2.11602226e-02,  4.15352061e-02,\n",
              "         -6.92324042e-02, -2.75356527e-02,  9.73042194e-03,\n",
              "         -7.08991960e-02,  9.29580908e-03, -1.97941475e-02,\n",
              "          2.92060953e-02,  4.80757952e-02,  1.80260055e-02,\n",
              "         -6.51646480e-02, -2.98081860e-02, -2.91780792e-02,\n",
              "          7.60703087e-02,  4.87935953e-02, -3.31642926e-02,\n",
              "          1.61458123e-02, -1.31451205e-04, -2.67255958e-02,\n",
              "         -6.42088661e-03, -1.48142735e-02,  2.20720638e-02,\n",
              "          2.81758467e-03, -2.31219199e-03, -1.48174623e-02,\n",
              "          4.40655202e-02, -9.12187621e-02,  6.04377761e-02,\n",
              "         -1.04248282e-02,  1.06799621e-02,  7.33398050e-02,\n",
              "         -1.46769052e-02, -1.51104974e-02, -1.87099688e-02,\n",
              "         -2.51277480e-02, -1.45520177e-02,  2.16571912e-02,\n",
              "          2.54967492e-02,  2.15571374e-02, -5.59970737e-03,\n",
              "         -7.36506842e-03,  1.23904524e-02, -1.86853316e-02,\n",
              "          8.60909838e-03, -2.25182343e-02, -9.64536238e-03,\n",
              "         -1.17721073e-02, -4.08401042e-02, -2.52085626e-02,\n",
              "          6.12639822e-03,  2.72095632e-02,  1.04894964e-02]]),\n",
              " 'documents': ['Operating the Climate Control System  Your Googlecar has a climate control system that allows you to adjust the temperature and airflow in the car. To operate the climate control system, use the buttons and knobs located on the center console.  Temperature: The temperature knob controls the temperature inside the car. Turn the knob clockwise to increase the temperature or counterclockwise to decrease the temperature. Airflow: The airflow knob controls the amount of airflow inside the car. Turn the knob clockwise to increase the airflow or counterclockwise to decrease the airflow. Fan speed: The fan speed knob controls the speed of the fan. Turn the knob clockwise to increase the fan speed or counterclockwise to decrease the fan speed. Mode: The mode button allows you to select the desired mode. The available modes are: Auto: The car will automatically adjust the temperature and airflow to maintain a comfortable level. Cool: The car will blow cool air into the car. Heat: The car will blow warm air into the car. Defrost: The car will blow warm air onto the windshield to defrost it.'],\n",
              " 'uris': None,\n",
              " 'data': None,\n",
              " 'metadatas': [None],\n",
              " 'included': [<IncludeEnum.embeddings: 'embeddings'>,\n",
              "  <IncludeEnum.documents: 'documents'>,\n",
              "  <IncludeEnum.metadatas: 'metadatas'>]}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Switch to query mode when generating embeddings.\n",
        "embed_fn.document_mode = False\n",
        "\n",
        "# Search the Chroma DB using the specified query.\n",
        "query = \"How do you use the touchscreen to play music?\"\n",
        "\n",
        "result = db.query(query_texts=[query], n_results=1)\n",
        "[all_passages] = result[\"documents\"]\n",
        "\n",
        "print(all_passages[0])"
      ],
      "metadata": {
        "id": "hlEQbiYKfslV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ce02ee6-aa38-4ce0-9d53-a453cbb3b5a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your Googlecar has a large touchscreen display that provides access to a variety of features, including navigation, entertainment, and climate control. To use the touchscreen display, simply touch the desired icon.  For example, you can touch the \"Navigation\" icon to get directions to your destination or touch the \"Music\" icon to play your favorite songs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_oneline = query.replace(\"\\n\", \" \")\n",
        "\n",
        "# This prompt is where you can specify any guidance on tone, or what topics the model should stick to, or avoid.\n",
        "prompt = f\"\"\"You are a helpful and informative bot that answers questions using text from the reference passage included below.\n",
        "Be sure to respond in a complete sentence, being comprehensive, including all relevant background information.\n",
        "However, you are talking to a non-technical audience, so be sure to break down complicated concepts and\n",
        "strike a friendly and converstional tone. If the passage is irrelevant to the answer, you may ignore it.\n",
        "\n",
        "QUESTION: {query_oneline}\n",
        "\"\"\"\n",
        "\n",
        "# Add the retrieved documents to the prompt.\n",
        "for passage in all_passages:\n",
        "    passage_oneline = passage.replace(\"\\n\", \" \")\n",
        "    prompt += f\"PASSAGE: {passage_oneline}\\n\"\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "U5al1-Jqf2zd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "416b0645-a672-4aeb-f57a-46cee5a6b9c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are a helpful and informative bot that answers questions using text from the reference passage included below.\n",
            "Be sure to respond in a complete sentence, being comprehensive, including all relevant background information.\n",
            "However, you are talking to a non-technical audience, so be sure to break down complicated concepts and\n",
            "strike a friendly and converstional tone. If the passage is irrelevant to the answer, you may ignore it.\n",
            "\n",
            "QUESTION: How do you use the touchscreen to play music?\n",
            "PASSAGE: Your Googlecar has a large touchscreen display that provides access to a variety of features, including navigation, entertainment, and climate control. To use the touchscreen display, simply touch the desired icon.  For example, you can touch the \"Navigation\" icon to get directions to your destination or touch the \"Music\" icon to play your favorite songs.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    contents=prompt)\n",
        "\n",
        "print(answer.text)"
      ],
      "metadata": {
        "id": "VsFQUE-uf5XG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}